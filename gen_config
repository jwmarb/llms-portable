#!/bin/bash

MODELS=$ROOT/models
LLAMA_SERVER=$ROOT/llama.cpp/build/bin/llama-server
VLLM=$(cat <<EOF
/usr/bin/docker run
      --gpus all
      -v ${MODELS}/.cache/huggingface:/root/.cache/huggingface
      -p 8375:8000
      --ipc=host
      vllm/vllm-openai:latest
EOF
)
DEFAULT_VLLM=$(cat <<EOF
proxy: http://localhost:8375
    cmd: >
      ${VLLM}
EOF
)
DEFAULT=$(cat <<EOF
proxy: http://localhost:4377
    cmd: >
      ${LLAMA_SERVER}
      --port 4377
      --flash-attn
      --cache-type-k q8_0
      --cache-type-v q8_0
      -ngl 99
EOF
)
DEFAULT_DRAFT=$(cat <<EOF
-ngld 99
      --draft-max 12
      --draft-min 1
      --draft-p-min 0.6
EOF
)

cat > $ROOT/llama-swap.yml <<EOF
models:
  exaone-deep-32b:
    ${DEFAULT}
      --model ${MODELS}/LGAI-EXAONE_EXAONE-Deep-32B-IQ3_XXS.gguf
      --ctx-size 32768
      ${DEFAULT_DRAFT}
      -md ${MODELS}/LGAI-EXAONE_EXAONE-Deep-2.4B-Q8_0.gguf
  exaone-deep-7.8b:
    ${DEFAULT}
      --model ${MODELS}/LGAI-EXAONE_EXAONE-Deep-7.8B-Q4_K_M.gguf
      --ctx-size 32768
      ${DEFAULT_DRAFT}
      -md ${MODELS}/LGAI-EXAONE_EXAONE-Deep-2.4B-Q8_0.gguf
  exaone-deep-2.4b:
    ${DEFAULT}
      --model ${MODELS}/LGAI-EXAONE_EXAONE-Deep-2.4B-Q8_0.gguf
      --ctx-size 32768
  phi-4-mini-instruct:
    ${DEFAULT}
      --model ${MODELS}/microsoft_Phi-4-mini-instruct-Q8_0.gguf
      --ctx-size 131072
  olmo-2-0325-32b:
    ${DEFAULT}
      --model ${MODELS}/OLMo-2-0325-32B-Instruct.i1-IQ3_S.gguf
      --ctx-size 4096
      ${DEFAULT_DRAFT}
      -md ${MODELS}/OLMo-2-1124-7B-Instruct-IQ4_XS.gguf
  granite-vision-3.2-2b:
    ${DEFAULT}
      --model ${MODELS}/ibm-granite_granite-vision-3.2-2b-Q8_0.gguf
      --ctx-size 131072
  reka-flash-3:
    ${DEFAULT}
      --model ${MODELS}/RekaAI_reka-flash-3-IQ4_XS.gguf
      --ctx-size 32768
  granite-3.2-8b:
    ${DEFAULT}
      --model ${MODELS}/ibm-granite_granite-3.2-8b-instruct-Q4_K_M.gguf
      --ctx-size 32768
      ${DEFAULT_DRAFT}
      -md ${MODELS}/ibm-granite_granite-3.2-2b-instruct-Q8_0.gguf
  phi-4:
    ${DEFAULT}
      --model ${MODELS}/phi-4-IQ4_XS.gguf
      --ctx-size 16384
  olympiccoder-32b:
    ${DEFAULT}
      --model ${MODELS}/open-r1_OlympicCoder-32B-IQ3_XXS.gguf
      --temp 0.7
      --top-k 50
      --top-p 0.95
      ${DEFAULT_DRAFT}
      -md ${MODELS}/Qwen2.5-Coder-0.5B-Instruct-Q8_0.gguf
      --ctx-size 32768
  llama3.2-reasoning-1b:
    ${DEFAULT}
      --model ${MODELS}/Reasoning-Llama-1b-v0.1-Q8_0.gguf
      --ctx-size 8192
      --temp 0.7
      --top-k 50
      --top-p 0.95
  llama3.1-nemotron-nano-8b:
    ${DEFAULT}
      --model ${MODELS}/Llama-3.1-Nemotron-Nano-8B-v1-Q4_K_M.gguf
      ${DEFAULT_DRAFT}
      -md ${MODELS}/Reasoning-Llama-1b-v0.1-Q8_0.gguf
      --ctx-size 131072
      --temp 0.7
      --top-k 50
      --top-p 0.95
  llama3.2-3b:
    ${DEFAULT}
      --model ${MODELS}/Llama-3.2-3B-Instruct-Q8_0.gguf
      ${DEFAULT_DRAFT}
      -md ${MODELS}/Llama-3.2-1B-Instruct-Q8_0.gguf
      --ctx-size 131072
  llama3.2-1b:
    ${DEFAULT}
      --model ${MODELS}/Llama-3.2-1B-Instruct-Q8_0.gguf
      --ctx-size 131072
  mistral-small-24b:
    ${DEFAULT}
      --model ${MODELS}/Mistral-Small-24B-Instruct-2501-IQ4_XS.gguf
      --ctx-size 32768
      --temp 0.15
  mistral-small-3.1-24b:
    ${DEFAULT}
      --model ${MODELS}/mistralai_Mistral-Small-3.1-24B-Instruct-2503-IQ4_XS.gguf
      --ctx-size 32768
      --temp 0.15
  gemma3-27b:
    ${DEFAULT}
      --model ${MODELS}/google_gemma-3-27b-it-IQ4_XS.gguf
      ${DEFAULT_DRAFT}
      -md ${MODELS}/google_gemma-3-1b-it-Q8_0.gguf
      --ctx-size 8192
  gemma3-12b:
    ${DEFAULT}
      --model ${MODELS}/google_gemma-3-12b-it-Q4_K_M.gguf
      ${DEFAULT_DRAFT}
      -md ${MODELS}/google_gemma-3-1b-it-Q8_0.gguf
      --ctx-size 32768
  gemma3-4b:
    ${DEFAULT}
      --model ${MODELS}/google_gemma-3-4b-it-Q8_0.gguf
      --ctx-size 131072
  gemma3-1b:
    ${DEFAULT}
      --model ${MODELS}/google_gemma-3-1b-it-Q8_0.gguf
      --ctx-size 32768
  qwen2.5-coder-7b:
    proxy: http://localhost:8371
    cmd: >
      ${LLAMA_SERVER}
      --port 8371
      --model ${MODELS}/Qwen2.5.1-Coder-7B-Instruct-Q4_K_M.gguf
      -md ${MODELS}/Qwen2.5-Coder-0.5B-Instruct-Q8_0.gguf
      --flash-attn
      --cache-type-k q8_0
      --cache-type-v q8_0
      -ngl 99
      -ngld 99
      --draft-max 20
      --draft-min 1
      --draft-p-min 0.6
      --ctx-size 16384
      --parallel 2
  gte-qwen2-1.5b-instruct:
    proxy: http://localhost:8372
    cmd: >
      ${LLAMA_SERVER}
      --port 8372
      --model ${MODELS}/gte-Qwen2-1.5B-instruct.Q8_0.gguf
      -ngl 99
      --embedding
      --ctx-size 32768
      --pooling cls
      # --ubatch-size 1536
      --ubatch-size 32768
      --parallel 8
  gte-qwen2-7b-instruct:
    proxy: http://localhost:8372
    cmd: >
      ${LLAMA_SERVER}
      --port 8372
      --model ${MODELS}/gte-Qwen2-7B-instruct-Q4_K_M.gguf
      -ngl 99
      --embeddings
      --ctx-size 32768
      --pooling rank
      --ubatch-size 4096
  readerlm-v2:
    proxy: http://localhost:8373
    cmd: >
      ${LLAMA_SERVER}
      --port 8373
      --model ${MODELS}/ReaderLM-v2.f16.gguf
      -ngl 99
      --flash-attn
      # --cache-type-k q8_0
      # --cache-type-v q8_0
      --ctx-size 262144
      --temp 0.0
      --top-k 50
      --repeat-penalty 1.13
      --presence-penalty 0.25
      --frequency-penalty 0.25
  multilingual-e5-large-instruct:
    proxy: http://localhost:8375
    cmd: >
      ${LLAMA_SERVER}
      --port 8375
      --model ${MODELS}/multilingual-e5-large-instruct-q8_0.gguf
      --embeddings
      -ngl 99
      --ctx-size 514
  multilingual-e5-large:
    proxy: http://localhost:8376
    cmd: >
      ${LLAMA_SERVER}
      --port 8376
      --model ${MODELS}/multilingual-e5-large-q8_0.gguf
      --embeddings
      -ngl 99
      --ctx-size 1024
      --ubatch-size 512
  multilingual-e5-large-cpu:
    proxy: http://localhost:8377
    cmd: >
      ${LLAMA_SERVER}
      --port 8377
      --model ${MODELS}/multilingual-e5-large-q8_0.gguf
      --embeddings
      --ctx-size 3072
      --parallel 6
      --ubatch-size 3072
      --prio 2
      --threads $(nproc)
profiles:
  coding:
    - "qwen2.5-coder-7b"
    - "multilingual-e5-large-cpu"
    - "multilingual-e5-large"
    - "readerlm-v2"
EOF

echo "Detected changes to gen_config"
